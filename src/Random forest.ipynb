{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import math\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
    "\r\n",
    "\r\n",
    "# opening the dataset\r\n",
    "dataset = pd.read_csv(\"../datasets/BitcoinHeistData.csv\")\r\n",
    "dataset\r\n",
    "\r\n",
    "# extracting licit addresses (41413 rows)\r\n",
    "licitAddresses = dataset.loc[dataset['label'] == 'white']\r\n",
    "\r\n",
    "# extracting illicit addresses (2875284 rows)\r\n",
    "IllicitAddresses = dataset.loc[dataset['label'] != 'white']\r\n",
    "\r\n",
    "# removing an outlier\r\n",
    "IllicitAddresses = IllicitAddresses.loc[IllicitAddresses['weight'] < 100]\r\n",
    "\r\n",
    "#taking a random 100000 rows of the Illicit addresses (so as to not have very imbalanced data)\r\n",
    "licit_subset = licitAddresses.sample(100000)\r\n",
    "\r\n",
    "# merging the two classes\r\n",
    "Full_Dataset = pd.concat([IllicitAddresses,licit_subset])\r\n",
    "DATA = Full_Dataset.reset_index()\r\n",
    "\r\n",
    "# adding a new illicit column : 0 for licit addresses and 1 for illicit\r\n",
    "Illicit = []*141412\r\n",
    "for i in range(141412):\r\n",
    "    if DATA['label'][i] == 'white' :\r\n",
    "        Illicit.append(0)\r\n",
    "    else :\r\n",
    "        Illicit.append(1)\r\n",
    "DATA['Illicit'] = Illicit\r\n",
    "\r\n",
    "DATA = DATA.sample(frac=1)\r\n",
    "DATA = DATA.reset_index()\r\n",
    "DATA = DATA.dropna()\r\n",
    "\r\n",
    "#droping unnecessary columns\r\n",
    "features = DATA.drop(['level_0','index','address','year','day','label'], axis=1)\r\n",
    "features\r\n",
    "\r\n",
    "\r\n",
    "labels = np.array(features['Illicit'])\r\n",
    "# Remove the labels from the features\r\n",
    "# axis 1 refers to the columns\r\n",
    "features= features.drop('Illicit', axis = 1)\r\n",
    "# Saving feature names for later use\r\n",
    "feature_list = list(features.columns)\r\n",
    "# Convert to numpy array\r\n",
    "features = np.array(features)\r\n",
    "# Split the data into training and testing sets\r\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\r\n",
    "\r\n",
    "print('Training Features Shape:', train_features.shape)\r\n",
    "print('Training Labels Shape:', train_labels.shape)\r\n",
    "print('Testing Features Shape:', test_features.shape)\r\n",
    "print('Testing Labels Shape:', test_labels.shape)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features Shape: (106059, 6)\n",
      "Training Labels Shape: (106059,)\n",
      "Testing Features Shape: (35353, 6)\n",
      "Testing Labels Shape: (35353,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Experiment #1\r\n",
    "\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "# Random search cross validation\r\n",
    "# this is a method to determine the best hyperparameters to use with the model\r\n",
    "\r\n",
    "\r\n",
    "# Number of trees in random forest\r\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\r\n",
    "# Number of features to consider at every split\r\n",
    "max_features = ['auto', 'sqrt']\r\n",
    "# Maximum number of levels in tree\r\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\r\n",
    "max_depth.append(None)\r\n",
    "# Minimum number of samples required to split a node\r\n",
    "min_samples_split = [2, 5, 10]\r\n",
    "# Minimum number of samples required at each leaf node\r\n",
    "min_samples_leaf = [1, 2, 4]\r\n",
    "# Method of selecting samples for training each tree\r\n",
    "bootstrap = [True, False]\r\n",
    "\r\n",
    "random_grid = {'n_estimators': n_estimators,\r\n",
    "               'max_features': max_features,\r\n",
    "               'max_depth': max_depth,\r\n",
    "               'min_samples_split': min_samples_split,\r\n",
    "               'min_samples_leaf': min_samples_leaf,\r\n",
    "               'bootstrap': bootstrap}\r\n",
    "\r\n",
    "\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
    "# Train the model on training data\r\n",
    "rf_random.fit(train_features, train_labels);\r\n",
    "\r\n",
    "# printing the best hyperparameters \r\n",
    "rf_random.best_params_\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# predictions = rf.predict(test_features)\r\n",
    "\r\n",
    "# # some metrics to evaluate the model\r\n",
    "# print(confusion_matrix(test_labels,predictions))\r\n",
    "# print(classification_report(test_labels,predictions))\r\n",
    "# print(accuracy_score(test_labels, predictions))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_estimators': 1577,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'bootstrap': True}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn import metrics\r\n",
    "\r\n",
    "\r\n",
    "# using the model with the best hyperparameters found before, to predict the test set\r\n",
    "best_rf = rf_random.best_estimator_\r\n",
    "pred = best_rf.predict(test_features)\r\n",
    "\r\n",
    "score = metrics.f1_score(test_labels, pred)\r\n",
    "print(\"F1 score :\", score)\r\n",
    "pscore = metrics.accuracy_score(test_labels, pred)\r\n",
    "print(\"accuracy : \", pscore)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score : 0.6043782441886707\n",
      "accuracy :  0.8016575679574577\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Experiment #2\r\n",
    "\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\r\n",
    "from numpy import mean\r\n",
    "\r\n",
    "# using a balanced random forest model, that makes up for how imbalanced the dataset is\r\n",
    "\r\n",
    "# define model\r\n",
    "model = BalancedRandomForestClassifier(n_estimators=1000,max_depth=20)\r\n",
    "# define evaluation procedure\r\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "# evaluate model using cross validation\r\n",
    "scores = cross_val_score(model, features, labels, scoring='roc_auc', cv=cv, n_jobs=-1)\r\n",
    "# summarize performance\r\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean ROC AUC: 0.839\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# feature_imp = pd.Series(model.feature_importances_,index=['length','weight','count','looped','neighbors','income']).sort_values(ascending=False)\r\n",
    "# feature_imp\r\n",
    "# sns.barplot(x=feature_imp, y=feature_imp.index)\r\n",
    "# # Add labels to your graph\r\n",
    "# plt.xlabel('Feature Importance Score')\r\n",
    "# plt.ylabel('Features')\r\n",
    "# plt.title(\"Visualizing Important Features\")\r\n",
    "# plt.legend()\r\n",
    "# plt.savefig(\"../imgs/Random forest features importance.jpg\")\r\n",
    "# plt.show()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotFittedError",
     "evalue": "This BalancedRandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b0d2d0b70263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weight'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'looped'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'neighbors'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'income'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_imp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_imp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Add labels to your graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature Importance Score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MACHINE LEARNING\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \"\"\"\n\u001b[1;32m--> 445\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs,\n",
      "\u001b[1;32m~\\.conda\\envs\\MACHINE LEARNING\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\MACHINE LEARNING\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This BalancedRandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('MACHINE LEARNING': conda)"
  },
  "interpreter": {
   "hash": "bafd9a389957e655fc4a14100b8046fbffdd0d44d41b67e95b7e280b879e2e60"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}