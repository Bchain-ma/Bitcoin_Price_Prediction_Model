{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import math\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
    "\r\n",
    "\r\n",
    "# opening the dataset\r\n",
    "dataset = pd.read_csv(\"../datasets/BitcoinHeistData.csv\")\r\n",
    "dataset\r\n",
    "\r\n",
    "# extracting licit addresses (41413 rows)\r\n",
    "licitAddresses = dataset.loc[dataset['label'] == 'white']\r\n",
    "\r\n",
    "# extracting illicit addresses (2875284 rows)\r\n",
    "IllicitAddresses = dataset.loc[dataset['label'] != 'white']\r\n",
    "\r\n",
    "# removing an outlier\r\n",
    "IllicitAddresses = IllicitAddresses.loc[IllicitAddresses['weight'] < 100]\r\n",
    "\r\n",
    "#taking a random 100000 rows of the Illicit addresses (so as to not have very imbalanced data)\r\n",
    "licit_subset = licitAddresses.sample(100000)\r\n",
    "\r\n",
    "# merging the two classes\r\n",
    "Full_Dataset = pd.concat([IllicitAddresses,licit_subset])\r\n",
    "DATA = Full_Dataset.reset_index()\r\n",
    "\r\n",
    "# adding a new illicit column : 0 for licit addresses and 1 for illicit\r\n",
    "Illicit = []*141412\r\n",
    "for i in range(141412):\r\n",
    "    if DATA['label'][i] == 'white' :\r\n",
    "        Illicit.append(0)\r\n",
    "    else :\r\n",
    "        Illicit.append(1)\r\n",
    "DATA['Illicit'] = Illicit\r\n",
    "\r\n",
    "DATA = DATA.sample(frac=1)\r\n",
    "DATA = DATA.reset_index()\r\n",
    "DATA = DATA.dropna()\r\n",
    "\r\n",
    "#droping unnecessary columns\r\n",
    "features = DATA.drop(['level_0','index','address','year','day','label'], axis=1)\r\n",
    "features\r\n",
    "\r\n",
    "\r\n",
    "labels = np.array(features['Illicit'])\r\n",
    "# Remove the labels from the features\r\n",
    "# axis 1 refers to the columns\r\n",
    "features= features.drop('Illicit', axis = 1)\r\n",
    "# Saving feature names for later use\r\n",
    "feature_list = list(features.columns)\r\n",
    "# Convert to numpy array\r\n",
    "features = np.array(features)\r\n",
    "# Split the data into training and testing sets\r\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\r\n",
    "\r\n",
    "print('Training Features Shape:', train_features.shape)\r\n",
    "print('Training Labels Shape:', train_labels.shape)\r\n",
    "print('Testing Features Shape:', test_features.shape)\r\n",
    "print('Testing Labels Shape:', test_labels.shape)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features Shape: (106059, 6)\n",
      "Training Labels Shape: (106059,)\n",
      "Testing Features Shape: (35353, 6)\n",
      "Testing Labels Shape: (35353,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Experiment #1\r\n",
    "\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "\r\n",
    "# Random search cross validation\r\n",
    "# this is a method to determine the best hyperparameters to use with the model\r\n",
    "\r\n",
    "\r\n",
    "# Number of trees in random forest\r\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\r\n",
    "# Number of features to consider at every split\r\n",
    "max_features = ['auto', 'sqrt']\r\n",
    "# Maximum number of levels in tree\r\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\r\n",
    "max_depth.append(None)\r\n",
    "# Minimum number of samples required to split a node\r\n",
    "min_samples_split = [2, 5, 10]\r\n",
    "# Minimum number of samples required at each leaf node\r\n",
    "min_samples_leaf = [1, 2, 4]\r\n",
    "# Method of selecting samples for training each tree\r\n",
    "bootstrap = [True, False]\r\n",
    "\r\n",
    "random_grid = {'n_estimators': n_estimators,\r\n",
    "               'max_features': max_features,\r\n",
    "               'max_depth': max_depth,\r\n",
    "               'min_samples_split': min_samples_split,\r\n",
    "               'min_samples_leaf': min_samples_leaf,\r\n",
    "               'bootstrap': bootstrap}\r\n",
    "\r\n",
    "\r\n",
    "rf = RandomForestClassifier()\r\n",
    "\r\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\r\n",
    "# Train the model on training data\r\n",
    "rf_random.fit(train_features, train_labels);\r\n",
    "\r\n",
    "# printing the best hyperparameters \r\n",
    "rf_random.best_params_\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# predictions = rf.predict(test_features)\r\n",
    "\r\n",
    "# # some metrics to evaluate the model\r\n",
    "# print(confusion_matrix(test_labels,predictions))\r\n",
    "# print(classification_report(test_labels,predictions))\r\n",
    "# print(accuracy_score(test_labels, predictions))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_estimators': 1577,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'bootstrap': True}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn import metrics\r\n",
    "\r\n",
    "\r\n",
    "# using the model with the best hyperparameters found before, to predict the test set\r\n",
    "best_rf = rf_random.best_estimator_\r\n",
    "pred = best_rf.predict(test_features)\r\n",
    "\r\n",
    "score = metrics.f1_score(test_labels, pred)\r\n",
    "print(\"F1 score :\", score)\r\n",
    "pscore = metrics.accuracy_score(test_labels, pred)\r\n",
    "print(\"accuracy : \", pscore)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score : 0.6043782441886707\n",
      "accuracy :  0.8016575679574577\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Experiment #2\r\n",
    "\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\r\n",
    "from numpy import mean\r\n",
    "\r\n",
    "# using a balanced random forest model, that makes up for how imbalanced the dataset is\r\n",
    "\r\n",
    "# define model\r\n",
    "model = BalancedRandomForestClassifier(n_estimators=1000,max_depth=20)\r\n",
    "# define evaluation procedure\r\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "# evaluate model using cross validation\r\n",
    "scores = cross_val_score(model, features, labels, scoring='roc_auc', cv=cv, n_jobs=-1)\r\n",
    "# summarize performance\r\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean ROC AUC: 0.839\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Experiment #3\r\n",
    "\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from imblearn.pipeline import Pipeline\r\n",
    "\r\n",
    "\r\n",
    "imbalanced_set = pd.concat([IllicitAddresses,licitAddresses])\r\n",
    "imbalanced_set = imbalanced_set.reset_index()\r\n",
    "imbalanced_set\r\n",
    "\r\n",
    "Illicit = []*2916696\r\n",
    "for i in range(2916696):\r\n",
    "    if imbalanced_set['label'][i] == 'white' :\r\n",
    "        Illicit.append(0)\r\n",
    "    else :\r\n",
    "        Illicit.append(1)\r\n",
    "imbalanced_set['Illicit'] = Illicit\r\n",
    "\r\n",
    "imbalanced_set = imbalanced_set.sample(frac=1)\r\n",
    "imbalanced_set = imbalanced_set.reset_index()\r\n",
    "imbalanced_set = imbalanced_set.dropna()\r\n",
    "\r\n",
    "#droping unnecessary columns\r\n",
    "features1 = imbalanced_set.drop(['level_0','index','address','year','day','label','Illicit'], axis=1)\r\n",
    "\r\n",
    "\r\n",
    "labels1 = np.array(imbalanced_set['Illicit'])\r\n",
    "# Remove the labels from the features\r\n",
    "# Saving feature names for later use\r\n",
    "# Convert to numpy array\r\n",
    "features1 = np.array(features1)\r\n",
    "\r\n",
    "# SMOTE + undersampling\r\n",
    "over = SMOTE(sampling_strategy=0.1)\r\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\r\n",
    "steps = [('o', over), ('u', under)]\r\n",
    "pipeline = Pipeline(steps=steps)\r\n",
    "# transform the dataset\r\n",
    "features_bal, labels_bal = pipeline.fit_resample(features1, labels1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dff = pd.DataFrame(labels_bal)\r\n",
    "# 287k rows of class 1\r\n",
    "dff.loc[dff[0] == 1]\r\n",
    "# 575k rows of class 2\r\n",
    "dff.loc[dff[0] == 0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        0\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "...    ..\n",
       "575051  0\n",
       "575052  0\n",
       "575053  0\n",
       "575054  0\n",
       "575055  0\n",
       "\n",
       "[575056 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575051</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575052</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575053</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575054</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575055</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575056 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "\r\n",
    "rff = RandomForestClassifier(n_estimators=1000,max_depth=20)\r\n",
    "# define evaluation procedure\r\n",
    "cvv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
    "# evaluate model using cross validation\r\n",
    "scoress = cross_val_score(rff, features_bal, labels_bal, scoring='roc_auc', cv=cvv, n_jobs=-1)\r\n",
    "# summarize performance\r\n",
    "print('Mean ROC AUC: %.3f' % mean(scoress))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-178485666bef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mscoress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_bal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_bal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcvv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# summarize performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean ROC AUC: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from numpy import mean\r\n",
    "\r\n",
    "# score using SMOTE\r\n",
    "print('Mean ROC AUC: %.3f' % mean(scoress))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean ROC AUC: 0.964\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# feature_imp = pd.Series(model.feature_importances_,index=['length','weight','count','looped','neighbors','income']).sort_values(ascending=False)\r\n",
    "# feature_imp\r\n",
    "# sns.barplot(x=feature_imp, y=feature_imp.index)\r\n",
    "# # Add labels to your graph\r\n",
    "# plt.xlabel('Feature Importance Score')\r\n",
    "# plt.ylabel('Features')\r\n",
    "# plt.title(\"Visualizing Important Features\")\r\n",
    "# plt.legend()\r\n",
    "# plt.savefig(\"../imgs/Random forest features importance.jpg\")\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('MACHINE LEARNING': conda)"
  },
  "interpreter": {
   "hash": "bafd9a389957e655fc4a14100b8046fbffdd0d44d41b67e95b7e280b879e2e60"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}